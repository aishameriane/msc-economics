---
title: "Importance sampling in VAR models"
author: "Aishameriane"

header-includes:
   - \usepackage{bigints}
   - \usepackage[brazil]{babel}
   - \usepackage{graphicx}
   - \usepackage{amsmath}
   - \usepackage{calrsfs}
date: "24 de maio de 2017"
output: 
  ioslides_presentation:
    incremental: true
    autosize: true
bibliography: references2.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Contents

1. Bayesian VAR with Stochastic volatility
2. Importance sampling
3. Importance sampling application in [@uhlig_1997]
4. Next steps

## Uhlig (1997)

- *"Bayesian vector autoregressions with stochastic volatility"*
- Random shocks from a multivariate Beta distribution in the error precision matrix

## Uhlig (1997) - A simple case

Consider the model given by:

\[
\begin{eqnarray}
y_t &= \beta y_{t-1} + h_t^{-\frac{1}{2}} \epsilon_t \qquad \text{with} \quad \epsilon_t \sim \mathcal{N}(0,1) \tag{01}\\
h_{t+1} &= h_t \frac{\vartheta_t}{\lambda} \qquad \text{with} \quad \vartheta_t \sim \mathcal{B}_1\left(\frac{\nu + 1}{2}, \frac{1}{2} \right) \tag{02}
\end{eqnarray}
\]

  - Given $\lambda$ and $\nu$ positive scalars
  - Choosing $\bar{b}_0 \in \mathbb{R}$, $n_0 > 0$, $s_0^2 > 0$ and a function $g_0(\beta)\geq 0$
  - The prior density will be
      - $\pi_0(\beta, h_1) \propto g_0(\beta)f_{NG}(\beta, h_1|\bar{b}_0, \lambda n_0, s_0, \nu)$

## Uhlig (1997) - A simple case (cont.)

The posterior density for the simple case is given by

\[
\begin{eqnarray}
\pi_T(\beta, h_{T+1}) \propto g_T(\beta) f_{NG}\left(\beta, h_{T+1}| \bar{\beta}_T, \lambda n_T, s_T, \nu \right)
\end{eqnarray}
\]

- Where
    - \[
      \begin{eqnarray}
      n_t =& \lambda n_{t-1} + y^2_{t-1}\\
      \bar{b}_t =& \left(\lambda \bar{b}_{t-1} n_{t-1} + y_t y_{t-1} \right)/n_t
      \end{eqnarray}
      \]
    - \[
      \begin{eqnarray}
      s_t =& \lambda s_{t-1} + \frac{\lambda}{\nu}e_t^2\left(1-\frac{y_{t-1}^2}{n_t} \right)\\
      \end{eqnarray}
      \]
    - \[
      \begin{eqnarray}
      e_t =& y_t - \bar{b}_{t-1}y_{t-1}\\
      g_t(\beta) =& g_{t-1}(\beta)\left(\left(\beta_t - \bar{b}_t\right)^2 n_t + \frac{\nu}{\lambda}s_t \right)^{-\frac{1}{2}}
      \end{eqnarray}
      \]

## Importance sampling (example)

Example from [@casella_MC]

We want, for $X \sim \mathcal{C}(0,1)$, estimate $\mathbb{P}(X \geq 2)$:

\[
\tag{xx}
p = \mathbb{P}(X \geq 2) = \int\limits_2^\infty \frac{1}{\pi(1+x^2)}dx
\]

Imagine that it is not possible to sample from (xx). We have some options that we will explore.

## Importance sampling (example - cont.){.smaller}

#### Method 1

\[\tag{24}
p \approx \hat{p}_1 = \frac{1}{m}\sum\limits_{j=1}^m \mathbb{I}_{X_j > 2}
\]

A variância do estimador $\hat{p}_1$ pode ser obtida da seguinte maneira:

\[\tag{25}
\begin{eqnarray}
Var[\hat{p}_1] =& \\
=& Var\left[\frac{1}{m}\sum\limits_{j=1}^m \mathbb{I}_{X_j > 2} \right] \\
=& \frac{1}{m^2} \sum\limits_{j=1}^m \left( Var[\mathbb{I}_{X_j > 2]} \right) \\
=&\frac{1}{m^2}mp(1-p) = \frac{p(1-p)}{m}
\end{eqnarray}
\]

E uma vez que $\mathbb{P}(X \geq 2)=$ `r round(1-pcauchy(2,0,1),2)`, a variância do estimador em (24) será dada por $Var[\hat{p}_1] =$ `r round((round(1-pcauchy(2,0,1),2)*round(pcauchy(2,0,1),2)),3)` $/m$.

## Importance sampling (example - cont.){.smaller}

#### Method 2

Considerando que a distribuição de Cauchy(0,1) é simétrica em torno do zero, uma estimativa para $p$ seria:

\[\tag{26}
p \approx \hat{p}_2 = \frac{1}{2m}\sum\limits_{j=1}^m \mathbb{I}_{|X_j| > 2}
\]
Com
\[\tag{27}
\begin{eqnarray}
Var[\hat{p}_2] =& \\
=& Var\left[\frac{1}{2m}\sum\limits_{j=1}^m \mathbb{I}_{|X_j| > 2} \right]= \frac{1}{4m^2} \sum\limits_{j=1}^m \left( Var[\mathbb{I}_{|X_j| > 2]} \right) \\
=&\frac{1}{4m^2}\cdot 2mp(1-2p) = \frac{p(1-2p)}{2m}
\end{eqnarray}
\]

E, novamente usando o fato que $\mathbb{P}(X \geq 2)=$ `r round(1-pcauchy(2,0,1),2)`, a variância do estimador em (25) será dada por $Var[\hat{p}_2] =$ `r round(round(1-pcauchy(2,0,1),2)*(1-2*round(1-pcauchy(2,0,1),2))/2,3)` $/m$.

## Importance sampling (example - cont.){.smaller}

#### Method 3

O método 3 é meio comprido e ainda estou vendo como colocar

## Importance sampling (example - cont.){.smaller}

#### Method 4

Idem método 3

## Importance sampling {.smaller}

- Como existem diversos estimadores de Monte Carlo, se torna um problema saber decidir qual das estimativas é a melhor. O critério para esta decisão será com base na variância do estimador.

- De acordo com [@rubinstein], a redução da variância pode ser vista como uma forma de utilizar conhecimento prévio sobre o problema. Em um extremo, quando não se sabe nada a respeito das densidades envolvidas, não é possível reduzir a variabilidade. Por outro lado, se temos total conhecimento do problema, a variância é zero e métodos de MC não seriam necessários. Em suas palavras: *"Variance reduction cannot be obtained from nothing; it is merely a way of not wasting information"*. 

- O objetivo é obter uma estimativa para:

    - \[\tag{14}
      I=\int g(x)dx\text{,}\quad \quad x \in D \subset \mathbb{R}^n
      \]

## Importance sampling (cont.){.smaller}

A ideia da amostragem por importância será concentrar a amostragem dos pontos, utilizando integração de Monte Carlo, nas regiões de $D$ que tem mais "importância", ao invés de amostrar igualmente de toda a região.

```{r, echo=FALSE}
library(ggplot2)
windowsFonts(xkcd=windowsFont("xkcd"))

p9 <- ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
        stat_function(fun = dbeta, args = list(2, 2),
                      aes(colour = "Beta 2,2 "), size = 1.5) +
        stat_function(fun = dunif, args = list(0, 1),
                      aes(colour = "U 0,1"), size = 1.5) +
         stat_function(fun = dunif, args = list(0, 5),
                      aes(colour = "U 0,5"), size = 1.5) +
        scale_x_continuous(name = "X",
                              breaks = seq(0, 5, 0.5),
                              limits=c(0, 5)) +
        scale_y_continuous(name = "Density") +
        ggtitle("Beta2,2 x U0,1 x U0,5") +
        scale_colour_brewer(palette="Set1") +
        labs(colour = "Distribution") +
        theme(axis.line = element_line(size=1, colour = "black"),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              plot.title=element_text(size = 20, family="xkcd"),
              text=element_text(size = 16, family="xkcd"),
              axis.text.x=element_text(colour="black", size = 12),
              axis.text.y=element_text(colour="black", size = 12))
p9
```

## Importance sampling (cont.)

Podemos reescrever (14) como:

\[\tag{15}
I  = \int \frac{g(x)}{m_X(x)}m_X(x) dx = \mathbb{E}_m\left[\frac{g(x)}{m_X(x)} \right]
\]

Onde $X$ é um vetor aleatório com densidade $m_X(\cdot)$ tal que $m_X(x) > 0 \ \forall \ x \in D$. A função $m_X(\cdot)$ é conhecida como **amostrador de importância**.

## Importance sampling (cont.)

Considere o estimador:

\[\tag{16}
\hat{I}_S = \frac{g(X)}{m_X(X)}
\]

Ele é não viesado para (15) e sua variância é dada por:

\[\tag{17}
Var[\hat{I}_S] = \int \frac{g^2(X)}{m_X(X)}dx - I^2
\]

Podemos então aproximar a integral dada em (15) pegando uma amostra aleatória $X_1, \ldots, X_n$ da densidade $m_X(x)$ e substituir seu valor na equação de média amostral:

\[\tag{18}
\theta = \frac{1}{n} \sum\limits_{i=1}^n \frac{g(X_i)}{m_X(X_i)}
\]

## Importance sampling (cont.)

**Teorema 4.3.1** 
A variância mínima para $\hat{I}_S$ é dada por:

\[\tag{19}
Var[\hat{I}_S] = \left(\int |g^2(X)|dx\right)^2 - I^2
\]

E ocorre quando a variável aleatória $X$ tem densidade:

\[\tag{20}
m_X(x) = \frac{|g(x)|}{\int |g(x)|dx}
\]

## Importance sampling application in [@uhlig_1997]

Aqui vou colocar o modelo geral do Uhlig e explicar a motivação de usar amostragem por importância. A posteriori já está toda bem definida no artigo, então essa parte não tenho que me estressar agora.


## Next steps

- Implement [@uhlig_1997] algorithm
- Simulate or get some data
- Write
- ~~Sleep~~

## References
