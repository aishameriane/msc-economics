---
title: "Importance sampling in VAR models"
author: "A. Schmidt"

header-includes:
   - \usepackage{bigints}
   - \usepackage[brazil]{babel}
   - \usepackage{graphicx}
   - \usepackage{amsmath}
   - \usepackage{calrsfs}
   - \usepackage{mathrsfs}
date: "24 de maio de 2017"
output: 
  ioslides_presentation:
    incremental: true
    autosize: false
    fig_width: 7
    fig_height: 6
    fig_caption: true
bibliography: references2.bib
---

## Contents

1. Bayesian VAR with Stochastic volatility
2. Importance sampling
3. Importance sampling application in [@uhlig_1997]
4. Next steps


## Uhlig (1997) - General model{.smaller}

The $Var(k)$-model with time-varying error precision matrices is given by the measurement equation

\[
\begin{eqnarray}\tag{01}
Y_t &= \beta_{(0)} C_t + \beta_{(1)} Y_{t-1} + \ldots + \beta_{(k)} Y_{t-k} + \mathscr{U}(H_t^{-1})' \epsilon_t
\end{eqnarray}
\]

\[
\begin{eqnarray}
\text{ or } &Y_t = B X_t + \mathscr{U}(H_t^{-1})'\epsilon_t
\end{eqnarray}
\]

\[
\begin{eqnarray}
\text{with}& \ \epsilon_t \sim \mathcal{N}(0,I_m)
\end{eqnarray}
\]

- and the state equation

- \[
\begin{eqnarray}\tag{02}
H_{t+1} = \mathscr{U}(H_t)'\Theta_t\mathscr{U}(H_t) \lambda^{-1}
\end{eqnarray}
\]

- \[
\begin{eqnarray}
\text{with }\Theta_t \sim \mathcal{B}_m\left(\frac{\nu + c + km}{2}, \frac{1}{2} \right)
\end{eqnarray}
\]

## Uhlig (1997) - General model (cont.){.smaller}

Assuming $\lambda$ and $\nu$ known, we have:

\[ \tag{03}
\pi_0 (B, H_1) \propto g_0(B) \ \mathcal{NW}(B, H_1| \bar{B}_0, \lambda N_0, S_0, \nu)
\]

- Which leads to

- \[ \tag{04}
\pi_T(B, H_{T+1}) \propto g_T(B) \ \mathcal{NW}(B, H_{T+1}| \bar{B}_T, \lambda N_T, S_T, \nu)
\]

- Where
    - $e_t = Y_t - \bar{B}_{t-1}X_t$
    - $N_t = \lambda N_{t-1} + X_t X_t'$
    - $\bar{B}_t = (\lambda \bar{B}_{t-1}N_{t-1} + Y_t X'_t)N_t^{-1}$
    - $S_t = \lambda S_{t-1} + \frac{\lambda}{\nu}e_t(1-X'_tN_t^{-1}X_t)e'_t$
    - $g_t(B) = g_{t-1}(B)\left|(B-\bar{B}_t)N_t(B-\bar{B}_t)' + \frac{\nu}{\lambda}S_t \right|^{-\frac{1}{2}}$
    
## Uhlig (1997) - The prior

- Prior selection: variant of the Minnesota Prior
    - $\bar{B}_0$ - random walk specification;
    - $\nu = 20$ (quaterly data) or $\nu = 60$ (monthly data)
    - $\lambda = \frac{\nu}{\nu+1}$
    - $g_0(B) = 1$
    
## Uhlig - The marginal posterior $p(B)$ {.smaller}

- If we integrate (04) over $H_{T+1}$, we find

- \[
\begin{eqnarray}
log(p(B)) =& \chi + \frac{1}{2} \sum_{i=1}^T \log \left|(B - \bar{B}_t)N_t(B-\bar{B}_t)' + (\nu+1)S_t \right|\\
& -\frac{l+\nu}{2} \log\left|(B - \bar{B}_T)N_T(B-\bar{B}_T)' + (\nu+1)S_T \right|
\end{eqnarray}
\]

    - Since it is not a known p.d.f., we need other techniques to sample from it


## Importance sampling (example)

[Example](https://htmlpreview.github.io/?https://github.com/aishameriane/msc-economics/blob/master/Bayesian-macro/article/self-study/importance-sampling.html) from [@casella_MC]

We want, for $X \sim \mathcal{C}(0,1)$, estimate $\mathbb{P}(X \geq 2)$:

\[
\tag{I-1}
p = \mathbb{P}(X \geq 2) = \int\limits_2^\infty \frac{1}{\pi(1+x^2)}dx
\]

Imagine that it is not possible to sample from (I-1) and [numerical methods](https://htmlpreview.github.io/?https://github.com/aishameriane/msc-economics/blob/master/Bayesian-macro/lecture_notes/int_num_exemplos.html) are needed. 

## Importance sampling {.smaller}

#### Method 1

- \[
p \approx \hat{p}_1 = \frac{1}{m}\sum\limits_{j=1}^m \mathbb{I}_{X_j > 2}
\]

- The variance of $\hat{p}_1$ can be calculated using $\frac{p(1-p)}{m}$:  $Var[\hat{p}_1] =$ `r round((round(1-pcauchy(2,0,1),2)*round(pcauchy(2,0,1),2)),3)` $/m$.

#### Method 2

- The $\mathcal{C}(0,1)$ distribution is symmetric around $0$. Then, another estimative of $\mathbb{P}(X \geq 2)$ is:

    - \[
    p \approx \hat{p}_2 = \frac{1}{2m}\sum\limits_{j=1}^m \mathbb{I}_{|X_j| > 2}
    \]

- With variance equal to $\frac{p(1-2p)}{2m} =$ `r round(round(1-pcauchy(2,0,1),2)*(1-2*round(1-pcauchy(2,0,1),2))/2,3)` $/m$.

## Importance sampling {.smaller}

#### Method 3

- There is a major drawback using methods #1 and #2
    - We want to sample from $[2, +\infty)$ but both strategies make draws in larger regions!
- Using $X \sim \mathcal{U}(0,2)$ it is possible to rewrite $p$:
    - \[ \tag{I-2}
    \begin{eqnarray}
    p =& \frac{1}{2} - \int\limits_0^2 \overbrace{\frac{2}{\pi(1+x^2)}}^{h(x)}\overbrace{\frac{1}{2}}^{\text{pdf of }X}dx = \frac{1}{2} - \int\limits_0^2 h(x) f_X(x) dx \\
    =& 1/2 - \mathbb{E}[h(X)]
    \end{eqnarray}
    \]
    
- Based on (I-2), a new estimator can be proposed:

    - \[
    \hat{p}_3 = \frac{1}{2} - \frac{1}{m} \sum\limits_{j=1}^m h(U_j) = \frac{1}{2} - \frac{1}{m} \sum\limits_{j=1}^m     \frac{2}{\pi}(1+U_j^2)
    \]
    
- $Var(\hat{p}_3) = \frac{1}{m} \left[\mathbb{E}(h^2(U))- \mathbb{E}(h(U))\right] = \ $ `r  round(round((2+5*atan(2))/(5*pi^2),4)-round(((1/pi)*atan(2))^2,4),4)` $/m$.    
    
## Importance sampling {.smaller}

```{r, out.width = "600px", echo=FALSE, fig.align= 'center'}
library(ggplot2)
windowsFonts(xkcd=windowsFont("xkcd"))

p9 <- ggplot(data.frame(x = c(0, 1)), aes(x = x)) +
        stat_function(fun = dbeta, args = list(2, 2),
                      aes(colour = "Beta 2,2 "), size = 1.5) +
        stat_function(fun = dunif, args = list(0, 1),
                      aes(colour = "U 0,1"), size = 1.5) +
         stat_function(fun = dunif, args = list(0, 5),
                      aes(colour = "U 0,5"), size = 1.5) +
        scale_x_continuous(name = "X",
                              breaks = seq(0, 5, 0.5),
                              limits=c(0, 5)) +
        scale_y_continuous(name = "Density") +
        ggtitle("Beta2,2 x U0,1 x U0,5") +
        scale_colour_brewer(palette="Set1") +
        labs(colour = "Distribution") +
        theme(axis.line = element_line(size=1, colour = "black"),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              plot.title=element_text(size = 20, family="xkcd"),
              text=element_text(size = 16, family="xkcd"),
              axis.text.x=element_text(colour="black", size = 12),
              axis.text.y=element_text(colour="black", size = 12))
p9
```

## Importance sampling {.smaller}

Formally, we want to obtain an estimate for:

\[ \tag{I4}
I=\int\limits_\Theta g(x)p(\theta|y)d\theta = \int\limits_\Theta \frac{g(x)p(\theta|y)}{m(\theta)}m(\theta) d\theta = \mathbb{E}_m\left[\frac{g(x)p(\theta|y)}{m(\theta)} \right]
\]

- Where $\theta$ is a random vector with p.d.f. $m(\cdot)$ such as $m(\theta) > 0 \ \forall \ \theta \in D \quad$. Then

    - \[
    I \approx \hat{I}_S(\theta) = \frac{1}{S} \sum\limits_{i=1}\frac{g(\theta^i)f(\theta^i)}{m_X(\theta)}
    \]
    
- is unbiased for (I-5) and its variance is given by $Var[\hat{I}_S] = \int g^2(\theta)\frac{f^2(\theta)}{m(\theta)}d\theta - I^2$

## IS application in [@uhlig_1997] {.smaller}

- The marginal distribution for $B$ is given by

- \[
\begin{eqnarray}
log(p(B)) =& \chi + \frac{1}{2} \sum_{i=1}^T \log \left|(B - \bar{B}_t)N_t(B-\bar{B}_t)' + (\nu+1)S_t \right|\\
& -\frac{l+\nu}{2} \log\left|(B - \bar{B}_T)N_T(B-\bar{B}_T)' + (\nu+1)S_T \right|
\end{eqnarray}
\]

- The following steps are proposed
    1. Find the maximum (numerically), $B^*$, and the Hessian of (xx) evaluated at $vec(B*)$, $J*$
    2. Use $B*$ and $J*$ as the parameters of a *t-distribution*
        * with $0< \nu* < T + l + \nu - ml$
    3. Sample $B_i$, $i=1,2,\ldots, n$ from
- \[ 
I(B) \propto \left(1+vec(B-B*)'\frac{-J*}{\nu*+ml}vec(B-B*) \right)^{-(\nu* + ml)/2}
\]

## IS application in [@uhlig_1997] {.smaller}

- The generated sample from $I(B)$ can be used in the conditional density for $H_{T+1}$
    - $H_{T+1}|B \sim \mathcal{W}_m(l+\nu,\Sigma)$
    - $\Sigma^{-1} = \lambda(B-\bar{B}_T)N_T(B-\bar{B}_T) + \nu S_T$

- And it is possible now to approximate (*under certain circunstances*) $\bar{\phi}=\mathbb{E}_{\pi_T}\left[\phi(B, H_{T+1}) \right]$ using:
    - \[ 
    \bar{\phi}_n = \frac{\sum_{i=1}^n \phi(B_i, H_{T+1,i})w(B_i)}{\sum_{i=1}^n w(B_i)}
    \]

## Next steps

  - Get data 
  - Implement [@uhlig_1997] algorithm
  - Write

```{r, out.width = "200px", echo=FALSE, fig.align= 'right'}
knitr::include_graphics("C:\\Users\\Aishameriane\\OneDrive\\Documentos\\Mestrado Economia\\Bayesiana - 2017-01\\Materiais artigo\\Estudo\\Seminario\\grumpy.png")
```

## References