---
title: "EIS in MSV Wishart Models"
author: "A. Schmidt, C. Piazza"
header-includes:
   - \usepackage{bigints}
   - \usepackage[brazil]{babel}
   - \usepackage{graphicx}
   - \usepackage{amsmath}
   - \usepackage{calrsfs}
   - \usepackage{accents}
date: "December 03, 2017"
output: html_document
bibliography: references.bib
---

\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
 
            autoNumber: "all",
            formatNumber: function (n) {return '9.'+n}
      } 
  }
});
</script>

# Introduction

The aim of this code is to estimate the wishart stochastic volatility model (MSV) proposed by [@philipov2006a] and [@philipov2006b] using efficient importance sampling (EIS), proposed by [@richard_zhang2007]. [Click here](https://htmlpreview.github.io/?https://github.com/aishameriane/msc-economics/blob/master/Thesis/EIS/EIS_-_Pilot_Applications.html) for a pilot application of EIS in R.

The code is based in Matlab code provided by Guilherme Moura and Richard Schnorrenberger.

# Open questions

1. What's the difference between `RandStream` ([Matlab](https://www.mathworks.com/help/matlab/ref/randstream.html)) and `set.seed()` ([R](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html))?

<span style="color:purple">Aisha:</span> I think the first creates a sequence of numbers that are stored in the memory. For example, `s = RandStream('mt19937ar','Seed',semente); % fix seed;` plus `RandStream.setGlobalStream(s);` create a single stream and designate it as the current global stream. I'm not sure what `semente` does.

2. Por que $\gamma$ aparece [aqui](https://github.com/aishameriane/msc-economics/blob/master/Thesis/EIS/EIS%20-%20vers%C3%A3o%20Guilherme.pdf) com e sem o subscrito $t$ (por exemplo, eq 14 está sem e na eq 15 tem $t$)?

3. O Guilherme mandou a seguinte mensagem: " _Outra coisa, quando eu estava mexendo com isso, percebi que para aumentar a dimensionalidade do problema teria que conseguir simplificar um pouco a regressão em (11), mas acabei não tendo tempo de me debruçar melhor sobre o problema e acabei programando na força bruta mesmo. Porém, note da definição de $\chi$ em (18) e da definição de $\zeta$ em (15) que intuições adicionais a respeito dos parâmetros_ $\gamma_{1,t}$,$\Gamma_t$ _podem ser obtidas se calcularmos os determinantes (dos dois lados!) para perceber de onde vem a informação para a "identificação" destes no problema de regressão._ "

<span style="color:purple">Aisha:</span> Vou deixar as equações aqui para depois voltar nelas:

A equação (11) é o problema de minimização do algoritmo de EIS:

\[
\hat{\gamma}_t (\theta) = 
\begin{aligned}
& \underset{\gamma_t}{\text{min}}
& & \sum\limits_{t=1}^S \left\{ln\ \chi\left(\tilde{\Omega}_t^i; \gamma_{t+1}\right) - c_t -ln\ \zeta\left(\tilde{\Omega}_t^i; \gamma_t\right) \right\} \qquad \text{(11)}
\end{aligned}
\]

A equação (18) é a expressão analítica para a constante de integração do período $t$ do amostrador EIS:

\[\tag{18}
\chi\left(\Omega_{t-1}; \gamma_t \right) \propto \frac{|S_{t-1}^*|^{\frac{\nu^*}{2}}}{|S_{t-1}|^{\frac{\nu}{2}}}
\]

E $\zeta$ é dado por:

\[\tag{15}
\zeta\left(\Omega_t; \gamma_t \right) \propto |\Omega_t|^{\frac{\gamma_{1,t}}{2}} \ \exp \ \left\{-\frac{1}{2}\ tr \ |\Gamma_t \cdot \Omega_t| \right\}
\]

Eu não entendi se o Guilherme quis dizer tirar o determinante de (18) e (15) e também não entendi como que isso ajuda na identificação dos parâmetros...

5. Tem diferença entre `repmat(matriz, [1 1 Neis])` e `repmat(matriz, [1, 1, Neis])`?

6. Qual a diferença entre `[cholSeis,p]  = cholcov(Seis(:,:,i,t))` e `[cholSeis,~]  = cholcov(Seis(:,:,i,t))`?

<span style="color:purple">Aisha:</span>Vide: https://www.mathworks.com/help/stats/cholcov.html (eu não entendo mesmo o papel do til).

7. Qual a diferença entre `W = wishrnd(Sigma,df)` e `W = wishrnd(Sigma,df,D)`?

<span style="color:purple">Aisha:</span> Vide: https://www.mathworks.com/help/stats/wishrnd.html
_W = wishrnd(Sigma,df) generates a random matrix W having the Wishart distribution with covariance matrix Sigma and with df degrees of freedom. The inverse of W has the Inverse Wishart distribution with parameters Tau = inv(Sigma) and df degrees of freedom._

_W = wishrnd(Sigma,df,D) expects D to be the Cholesky factor of Sigma. If you call wishrnd multiple times using the same value of Sigma, it's more efficient to supply D instead of computing it each time._

Especificamente, não entendi a linha 42 de `lik_KChi_initial.m` que diz:

```{}
Veis(:,:,i,t) = wishrnd(Seis(:,:,i,t),df_eis,cholSeis);  % sample from EIS sampler; 
```

Estou desconfiada que apenas deixa mais rápido...

8. Esse comentário está na linha 65 do código `lik_KChi_initial.m`:

_Note that there is a log-likelihood function for each period t and replication i, and that *the mean of the observable multivariate data is equal to zero*_

Por que a média é zero?

9. Esse comentário está na linha 66 do código `lik_KChi_initial.m`:

_Obs: +0.5*log(det(Veis(:,:,i,t))) = -0.5*log(det(inv(Veis(:,:,i,t)))), but the first one is faster to run_

Isso acontece porque Veis é simétrica?

10. Na função `lik_KChi_initial.m` na linha 74 tem

```{}
 mt(t,i)   = exp(lmt(t,i));
```

Porém essa quantidade não é usada em lugar nenhum.
Também não entendi o que são os `lmt`, `lpt`, `lgt`.

11. Não entendi bem o que isso faz:

```{}
df_eis = [bet(end,:)'+ df_p1; df_p1]
```

12. Não entendi onde que essa coisa aparece na equação (17) nem o que ela faz

```{}
Seis(:,:,i,t) = S(:,:,i,t)-1/(1+trace(EISmat*S(:,:,i,t)))*S(:,:,i,t)*EISmat*S(:,:,i,t);  
% scale matrix of the EIS sampler. See Eq. (17). This code is faster than inv(EISmat+inv(S(:,:,i,t)));
```

13. No código `lik_KChi_initial` quando faz a decomposição de Cholesky, se usa uma matriz auxiliar `SeisSPD` (linha 37), mas no código `lik_KChi_R` isso não é feito (linha 63):

`lik_KChi_initial`
```{}
[cholSeis,p]  = cholcov(Seis(:,:,i,t));         
% cholesky decomposition for the scale matrix of the EIS sampler; Aisha: [T,num] = cholcov(SIGMA) returns the number num of %negative eigenvalues of SIGMA, and T is empty if num is positive. If num is zero, SIGMA is positive semi-definite. If SIGMA is %not square and symmetric, num is NaN and T is empty.
        if p~=0                                         % logical operator which is set to logical 1 (true) when p is not equal %to 0. Note that if p==0, Seis is positive semi-definite;
            SeisSPD = nearestSPD(Seis(:,:,i,t));        % if EIS scale matrix is not positive definite, nearestSPD pick the %closest Symmetric Positive Definite matrix to Seis;
            [cholSeis,~]  = cholcov(SeisSPD);
        end
```


`lik_KChi_R`
```{}
[cholSeis,p]  = cholcov(Seis(:,:,i,t));     % cholesky decomposition for the scale matrix of the EIS sampler;
            if p~=0                         % if EIS scale matrix is not positive definite, nearestSPD pick the closest Symmetric Positive Definite matrix to Seis;
                Seis(:,:,i,t) = nearestSPD(Seis(:,:,i,t));
                [cholSeis,~]  = cholcov(Seis(:,:,i,t));
            end    
```



# Solved questions

4. No código de `lik_KChi_initial` tem uma função `par()` que não encontrei o que ela faz, e quando tentei rodar no meu matlab ela deu erro:

```{}
d = par(1);
v = par(2);
> Undefined function 'par' for input arguments of type 'double'.
```

Depois ela aparece de novo:

```{}
cC(inds) = par(3:Nb+2);                         % form the C^{1/2} matrix built before;
```

<span style="color:purple">Aisha:</span> É um vetorzinho chamado `par` e o que está entre parênteses é a posição.

# Code initialization

<span style="color:purple">*Aisha's remark*: I'm not sure about the order of the topics, when we finish the implementation we can discuss what is better. I think we should mantain the markdown file for documentation purposes but run the code in a R regular file, because *.rmd files are a little slower to compile.</span>

The observational density is given by

$$p(y_t|\Sigma_t)\sim N(0,\Sigma_t)$$

Densidade de transição dos estados

$$p(\Sigma_{t}^{-1}|\Sigma_{t-1}^{-1},v,d,C)\sim Wishart(v,S_{t-1})$$
$$S_{t-1}=\frac{1}{v}(C^{1/2})(\Sigma_{t-1}^{-1})^{d}(C^{1/2})^{\prime}$$
The basic idea is to assume that [@philipov2006a] have correctly predicted the data generating process (DGP) and use theis estimates to generate new data. Below are the interaction between Aisha and Richard (from October, 2017):

<span style="color:pink">Aisha:</span> " _Minha primeira dúvida é quais dados você usou pra rodar o código. Eu vi que você usa uma especificação similar ao Phillipov e Zhang, com 240 variáveis, mas daí tem só 3 variáveis independentes, e no artigo deles fala em 5. Tem um código ali que faz simulação de valores, fiquei na dúvida se os seus resultados são direto com valores simulados ou se você tentou estimar usando a mesma base do artigo. Caso tenha sido a do artigo, você tem ela aí? Eu fui olhar no site que eles mencionam que pegaram, mas tem uma enormidade de bases e não achei ainda qual é a que o Phillipov pegou._ "

<span style="color:green">Richard:</span> " _1) A ideia do Guilherme é a seguinte: assumindo que o modelo estimado do Philipov e Glickman (2006) (Tabela 4 do apêndice se não me engano) é um bom modelo para aquela amostra, vamos assumir que este modelo seja o nosso processo gerador de dados. Poderíamos assumir qualquer PGD, mas aquele estimado por Philipov e Glickman (2006) parece ser uma boa representação daquela realidade, logo é uma boa pegar ele como o nosso PGD. Por isso que geramos (simulamos) as nossas realizações com base nesse PGD. Primeiramente o número de variáveis dependentes é 3 (modelo trivariado) porque o Guilherme quer ir testando a eficiência do código para este caso trivariado. Se os resultados forem positivos para o caso trivariado aumentamos para o caso de 4 e 5 variáveis, e em seguida aplica-se para dados do mercado financeiro. Por isso, de início o código só contempla a parte 3x3 da matriz invC. Portanto, a seção seguinte simula as realizações para aquele PGD, fixando uma semente de forma que as mesmas pseudo-realizações são simuladas toda vez que rodamos o código. 240 é o número de observações simuladas, o tamanho do nosso T. _ " 

## Loading packages
```{r, warning = FALSE, message= FALSE, eval = FALSE}
list.of.packages <- c("ggplot2", "MASS", "matrixcalc", "logOfGamma")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(ggplot2, quietly = TRUE)
library(MASS, quietly = TRUE)
library(matrixcalc, quietly = TRUE)
library(logOfGamma, quietly = TRUE)
```


## Reading Phillipov's Data

```{r, eval = FALSE, echo = FALSE}
dados <- read.delim("C:\\Users\\Aishameriane\\OneDrive\\Documentos\\Mestrado Economia\\Dissertação\\Pesquisa - EIS - Richard\\Meus códigos\\5_Industry_Portfolios.txt", sep = ";")
head(dados)
summary(dados[,-1])
dados[,2] <- as.numeric(dados[,2])
dados[,3] <- as.numeric(dados[,3])
dados[,4] <- as.numeric(dados[,4])
dados[,5] <- as.numeric(dados[,5])
dados[,6] <- as.numeric(dados[,6])
```


## Initial parameters and variables

```{r, eval = FALSE}
K     <- 3      # Number of observable variables
Tesao <- 240    # Number of time periods (it is consistent with P&G Multivariate...)
N     <- 500    # MC sample size (S from our original notation)
burn  <- 0      # Burn in ----- onde que usa?
d     <- 0.5    # Persistence parameter
v     <- 14     # Degrees of freedon from Wishart transition density ------ como decidir?
iv    <- 1/v

aux   <- c(0.0238, 0.0057, 0.0145, 0.0057, 0.0239, 0.0056, 0.0145, 0.0056, 0.0330)
invC  <- matrix(aux, ncol = 3, nrow = 3)   # From P&G page 321
# Obs: The inverse of C has a direct relation to the covariance matrix, while C relates to the precision matrix (inverse covariance)
## In the article, this matrix is called A

C     <- solve(invC)
cC    <- t(chol(C)) # produce a lower triangular matrix C^{1/2} (cC), so that C=C^{1/2}*C^{1/2}' = cC %*% t(cC);
# Chol provides the upper Chol Decomposition, that's why I'm transposing

zeroK <- as.vector(rep(0,K))
aux   <- matrix(1, ncol = K, nrow = K) 
aux[!lower.tri(matrix(1, ncol = K, nrow = K), diag = TRUE)] <- 0 
aux2 <- matrix(seq(1,K*K,1), ncol = K, nrow = K)
inds <- as.vector(aux2[!aux == 0]) # Index from covariance matrix to EIS parameters

semente <- set.seed(123456789)

# Initial States
Vini <- solve(diag(K)*0.15)  # What is this?
V0   <- Vini

# Generate data
Vtrue      <- rep(matrix(NaN, ncol = K, nrow = K),Tesao)    # NaN(1,2,3) in Matlab creates 3 1\times2 matrices filled with NaN
dim(Vtrue) <- c(K, K, Tesao)                            # So first I've created the elements and then I adjust the dimension
yT         <- matrix(NaN, nrow = T, ncol = K)
# lik_ratio <- matrix(NaN, nrow = T, ncol = 1)          # This line is commented in Matlab as well
#s          <- # I have to wait for Richard to understand the lines using RandStream

jit(1)  # turn on just-in-time compilation for faster loops
for (t in 1:Tesao){
  # Scale matrix
  S  <- cC%*% (V0^d) %*% t(cC)*iv                        # Draw from the Scale Matrix
  # Note that this loop is sampling one single realization from the variable of interest for every period t
  
  # Data Generating Process
  # rWishart(n, df, Sigma) - Generate n random matrices, distributed according to the Wishart distribution with parameters Sigma and df

  V1 <- rWishart(1, v, S)
  Vtrue[,,t] <- V1
  
  # Observable multivariate data
  # mvrnorm(n = 1, mu, Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
  # Produces one or more samples from the specified multivariate normal distribution.
  
  Y <- mvrnorm(n = 1, mu = zeroK, Sigma = solve(matrix(V1, ncol = K, nrow = K))) # Samples of observables drawn from a zero mean multivariate normal distribution with covariance matrix \Sigma_{t}; 
  yt[t,] <- Y
  # lik_ratio[t] <- ISratioWishart(yt(t,:),S,v); # This line is commented in Matlab
  V0 <- V1          # Note that V1_{t} is used as V0 for period t+1. That is, V1_{t} enters in S from the next period as V0_{t+1};
}
```

# Estimation

## EIS Procedure

## EIS Algorithm

This part requires 2 main functions: `lik_KChi_initial()` and `lik_KChi_R()`.

<span style="color:green">Richard:</span> " _Nesta seção queremos estimativas da verossimilhança para nosso modelo considerado. Por isso que os parâmetros são fixos, pois queremos montar uma série de verossimilhanças estimadas para o mesmo PGD. Por outro lado, mudamos as sementes para construir a série de verossimilhanças estimadas e assim poder analisar o erros-padrão de MC._

_O que precisa tomar cuidado nesta seção é a diferença entre as rotinas_ " lik_KChi_initial" _e_ "lik_KChi_R". _Ambas rotinas estimam nossos parâmetros de interesse (matrizes de precisão) com a técnica EIS, mas a rotina_ "lik_KChi_initial" _assume um amostrador inicial com parâmetros_ $\gamma_{t}=0$. _Isso quer dizer que o amostrador inicial assumido não explora informação de otimizações EIS (que tem como função trazer informação contemporânea sobre a volatilidade contida naquela distribuição conjunta das observáveis um passo a frente - em_ $t+1$ _)._

_Por outro lado a rotina _"lik_KChi_R" _realiza as otimizações EIS, e assim aproveita/explora informações sobre a volatilidade corrente presente naquela distribuição conjunta de um passo a frente das observáveis. Assim, o objetivo é comparar os resultados de_ "lik_KChi_initial" _e_ "lik_KChi_R" _para ver se a incorporação de otimizações EIS melhora as estimativas._ "

### Implementation of `lik_KChi_initial()`

The following explanation was copied from Richard's notes. OBs: Due to Markdown limitations, I couldn't implement the `\ubar{}` in the formulaes, so I'm using `\bar` instead.

In order to analyze statistical gains from using the EIS optimization method, an initial sampler can be constructed based on the kernel:

$$k(\bar{\Omega}_t; \gamma) = g(y_t|\Omega_t;\theta) \cdot p(\Omega_t | \Omega_{t-1}; \theta) \cdot \zeta(\Omega; \gamma_t)\tag{14}$$
by setting $\gamma_t = 0 \ \forall \ t$, which yields the following initial degrees of freedom and scale matrix of $m(\cdot)$:
\[ 
\begin{aligned}
\nu^{*\text{INI}} = \nu + 1 \qquad \text{and} \qquad S_{t-1}^{*\text{INI}} = \left[y_t \cdot y_t' + S_{t-1}^{-1} \right]^{-1}
\end{aligned}
\]

Note that $S_{t-1}^{*\text{INI}}$ incorporates contemporaneous information on the observables $y_t$ (through $y_t \cdot y_t'$), but it can not explore information from EIS optimizations (<span style="color:red">why not?</span>), which are not transferred to the importance sampler $m(\cdot)$. More specifically, the initial sampler does not incorporate information on $\Omega_t$ contained in $y_{t+1}$, because EIS parameters $\gamma_t$ cannot transfer information related to the EIS smoother through matrix $\Gamma_t$ since $\gamma_t = 0 \ \forall \ t$.

Thus, the initial sampler is used to draw $N$ trajectories of the latent precision matrixes $\left\{\left\{\tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$ which do not incorporate information on the dynamics of $\Omega_t$ contained in $f(y_{t+1}, \Omega_{t+1}|\bar{\Omega}_t, \bar{y}_t; \theta)$. For this reason, the initial sampler is used with comparison purposes.

Code `lik_KChi_initial` approximates the log-likelihood function using an approximation given by:

\[\tag{08}
\tilde{L}_S(\theta|\bar{y}) = \frac{1}{S} \sum\limits_{i=1}^S \left\{\prod\limits_{t=1}^T \left[\frac{f(y_t, \tilde{\Omega}_{t}^i| \tilde{\bar{\Omega}}_{t-1}^i), \bar{y}_{t-1}; \theta}{m\left(\tilde{\Omega}_t^i| \tilde{\bar{\Omega}}_{t-1}^i; \gamma_t \right)} \right] \right\}
\]

using the precision matrices $\left\{\left\{\tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$ sampled with the initial sampler:

a. The initial sampler is initialized using initial values for the precision matrix $\Omega_0$ at time $t=1$, the lower triangular matrix $A^{1/2}$ containing the intertemporal sensitivity parameters of $\Omega_t$, the persistence parameter $d$ and the degrees of freedom $\nu$ from the $k$-dimensional Wishart transition density. For example, the initial sampler can use values for $A^{1/2}$, $d$ and $\nu$ which follow the parameterization stated in Table 4 from [@philipov2006a] (<span style="color:red">colocar a tabela depois</span>);

b. $N$ draws of $S_t$, the scale matrix of the Wishart transition density, are computed $\forall \ t = 1,\ldots, T$, using the initial parameterization defined above. Note that for $t=1$, the $N$ computed scale matrices are equal once the initial $\Omega_0$ is the same for the entire MC sample. Next, $N$ draws of $S_{t-1}^{*\text{INI}}$, the scale matrix of the EIS sampler, can be computed via equation 19 (<span style="color:red">a equação 19 no PDF é lá do outro algoritmo, eu acho que ele quis dizer que era a equação que tem o $\nu^{*\text{INI}}$ e o $S_{t-1}^{*\text{INI}}$</span>).

c. Once a sample of size $N$ of scale matrices $S_t$, $\left\{\left\{\tilde{S}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$, are computed, the computation of $S_{t-1}^{*\text{INI}}$ is straightforward and one can sample $N$ trajectories of $\left\{\left\{\tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$ from the EIS sampler $\Omega_t|\nu^{*\text{INI}}, \Omega_{t-1} \sim \mathcal{W}_k\left(\nu^{*\text{INI}}, S_{t-1}^{*\text{INI}}\right)$;

d. Ultimately, code `lik_KChi_initial` computes the IS-MC estimate of the log-likelihood of IS ratio $\varphi(y_t, \Omega_t; \theta, \gamma)$ for the entire sample period and through the MC sample, using:

    (i). $\left\{\left\{\tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$ for the measurement density;
    
    (ii). $\left\{\left\{\tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N, \left\{\left\{\tilde{S}_t^i \right\}_{t=1}^T \right\}_{i=1}^N$ and $\nu$ for the state transition density;
    
    (iii.) $\left\{ \left\{ \tilde{\Omega}_t^i \right\}_{t=1}^T \right\}_{i=1}^N, \ \left\{\left\{ S_{t-1}^{*\text{INI}} \right\}_{t=1}^T \right\}_{i=1}^N$ and $\nu^{*\text{INI}}$ for the state transition density  <span style="color:red">Aisha: Aqui eu não sei se não seria $S_{t-1}$ porque no ponto c ele diz que vai amostrar de $\mathcal{W}_k\left(\nu^{*\text{INI}}, S_{t-1}^{*\text{INI}}\right)$</span>.
    
Function `logwishart.m`: see Anderson (2003) _An Introduction to Multivariate Statistics_ page 252.

```{r, eval = FALSE}
logwishart <- function(X,Omega,v) {
  n  <- nrow(X)
  nl <- ncol(X)
  
  
  if (n != nl) {
    stop("X must be a square matrix")
  }
  
  Arg    <- (v-n+1):v
  
  const  <- log(det(Omega)) * (-v/2) - (v*n*0.5) * log(2) - ((n*(n-1))/4) * log(pi) - sum(gammaln(Arg/2))
  kernel <- log(det(X)) * ((v-n-1)/2) - 0.5*sum(diag(solve(Omega, X)))
  
  lny <- const + kernel
  
  return(list(lny = lny, const = const))
}

```
 
```{r, eval = FALSE}
lik_KChi_initial <- function(par,yt,Vini,N,adj,semente) {
  
  Tesao <- nrow(yt)
  K     <- ncol(yt)
  
  d  <- par[1]
  v  <- par[2]
  iv <- 1/v
  
  # O S depende do randstream que eu não entendi como funciona ainda...
  s  <- RandStream('mt19937ar', 'Seed', semente) # Fix seed
  RandStream.setGlobalStream(s)
  
  Neis   <- N
  # Set a space for sampling N draws of the k \times k precision matrix \Sigma_{t}^{-1} for each period t;
  Veis   <- array(data = NA, dim = c(K,K,Neis, Tesao)) 
  Seis   <- Veis
  S      <- Veis
  # degrees of freedom for the Wishart distribution;
  df_eis <- v+1
  # Same as find(tril(ones(K))) but more complicated :P
  # Index from covariance matrix to EIS parameters;
  inds   <- which(matrix(as.numeric(lower.tri(matrix(1, ncol = K, nrow = K), diag = TRUE)), ncol = K, nrow = K) != 0)
  # Number of elements to estimate from the covariance matrix;
  Nb     <- length(inds)
  zeroK  <- as.vector(rep(0,K))
  
  # Form the C^{1/2} matrix built before;
  cC[inds] <- par[3:(Nb+2)]  
  # Transform the matrix cC into a matrix of size K \times K
  cC       <- matrix(cC, ncol = K, nrow = K)
  # Prepare the initial covariance matrix for N replications
  VINI     <- repmat(Vini,[1,1,Neis]) ###### ESSA LINHA NÃO FUNCIONA # repmat function repeats a matrix, but I'm not sure how -- see: https://www.mathworks.com/help/matlab/ref/repmat.html
  
  #Sample from closed form solution sampler:
  ###### I HAVE TO COME BACK HERE LATER
  V0 <- VINI;
  s  <- RandStream('mt19937ar','Seed',semente);     # fix seed
  RandStream.setGlobalStream(s);
  
  jit(1)  # turn on just-in-time compilation for faster loops
  for (t in 1:T) {
    jit(1)  # turn on just-in-time compilation for faster loops
    for (i in 1:Neis){
      S[,,i,t] <- cC %*% ((V0[,,i,t])^d) %*% t(cC) * iv
      EISmat   <- t(yt[t,]) %*% yt[t,]
      #scale matrix of the efficient importance sampler m(\cdot)
      Seis[,,i,t]     <- solve(EISmat + solve(S[,,i,t]))
      # I had to adapt the testing for positive definiteness, because I don't have an equivalent function to cholcov()
      if (is.positive.definite(Seis[,,i,t], tol=1e-8)) {
        # cholesky decomposition for the scale matrix of the EIS sampler
        cholSeis <- chol(Seis[,,i,t])
      } else {
        # if EIS scale matrix is not positive definite, nearestSPD pick the closest Symmetric Positive Definite matrix to Seis
        SeisSPD  <- nearPD(Seis[,,i,t]) #### I HAVE TO COME BACK HERE LATER AND WRITE THE FUNCTION
        cholSeis <- chol(SEISSPD)
      }
   
      Veis[,,i,t] <- rWishart(1, df_eis, Seis[,,i,t]) # Aisha: Probably I'll not need the chol decomposition from before, since it appears to only enter in line 42
      # set the next period V0 precision matrix since the scale matrix S_{t} depends on the precision matrix of the previous period Sigma_{t-1};
      # Why t<T? Because at time T, V0[:,:,i,t] comes from T-1, and the value of V0[:,:,i,T] is not necessary, once the scale matrix S_{T+1} is not simulated;
      
      if ( t < T) {
        V0[, ,i, t+1] <- Veis[, , i, t]
      }
    }
  }
  
  # Evaluating the EIS integrand
  
  ratio <- matrix(NA, nrow = T, ncol = N)
  gt    <- ratio
  lgt   <- gt
  lpt   <- gt
  #pt   <- gt # This was already commented
  lmt   <- gt
  mt    <- gt
  
  jit(1)  # turn on just-in-time compilation for faster loops
  for (t in 1:T){
    jit(1)  # turn on just-in-time compilation for faster loops
    for (1 in 1:N) {
        # Measurement Density
        # gt(t,i)  = mvnpdf(yt(t,:),zeroK, inv(Veis(:,:,i,t))); # This was already commented
        # lgt(t,i) = log(gt(t,i)); # This was already commented
        # log(gt(t,i)); # This was already commented
        
      # Note that there is a log-likelihood function for each period t and replication i, and that the mean of the observable multivariate data is equal to zero; ### WHY?
      # Obs: +0.5*log(det(Veis(:,:,i,t))) = -0.5*log(det(inv(Veis(:,:,i,t)))), but the first one is faster to run;
      lgt[t,i] <- -K/2*log(2*pi) + 0.5*log(det(Veis[,,i,t])) - 0.5*yt[t,] %*% Veis[,,i,t] %*% t(yt[,t])
      
      # State Transition Density
      lpt[t,i] <- logwishart(Veis[,,i,t], S[,,i,t],v)
      # pt <- exp(lpt[t,i]) # This was already commented
      
      # Importance Sampler
      lmt[t,i] <- logwishart(Veis[,,i,t], Seis[,,i,t], df_eis)
      mt[t,i]  <- exp(lmt[t,i])
      
      # IS ratio
      ratio[t,i] <- (lgt[t,i]+lpt[t,i]-lmt[t,i])
    }
  }
  
  # adj <- mean(mean(ratio)) # This was already commented
  
  lik    <- mean(exp(sum(ratio - adj)))
  loglike <- log(lik) + Tesao * adj 
  
  return(loglike) # I changed from loglik to loglike because R has a function named loglik.
}

```

### Implementation of `lik_KChi_R()`

```{r}
lik_Kchi_R <- function(par, yt, Vini, N, adj, semente){
  # Parameters
  Tesao <- nrow(yt) # number of periods;
  K     <- ncol(yt) # number of series;
  # Global Persistence Parameter
  # d <- par[1]^2/(1+par[1])^2 # This line was already commented
  d <- pa[1]
  # Degrees of freedom
  # v <- k + abs(par[2])
  v <- par[2]
  # MC sample size
  Neis <- N
  iv   <- 1/v
  
  
  ## Define some variables:
  # Dependent variable for EIS regression
  Y    <- matrix(NA, nrow = Neis, ncol = 1)
  # Array to collect draws from the importance sampler
  Veis <- array(data = NA, dim = c(K,K,Neis, Tesao))
  Seis   <- Veis
  S      <- Veis
  # degrees of freedom for the Wishart EIS initial sampler. See eq (17);
  df_p1 <- v+1 # It is not eq (17), it is eq(19) (the equation between 18 and 20)
  
  # Code to recover triangular matrix cC from the vector of parameters:
  
  # index from covariance matrix to EIS parameters (auxiliary parameters)
  inds   <- which(matrix(as.numeric(lower.tri(matrix(1, ncol = K, nrow = K), diag = TRUE)), ncol = K, nrow = K) != 0)
  # index from a diagonal KxK matrix;
  ind_   <- which(diag(K) != 0) 
  indd   <- matrix(0, nrow = 1, ncol = K+1)
  # number of estimated parameters from the covariance matrix or from the \Gamma matrix filled with (k^2+k)/2 auxiliary parameters
  Nb     <- length(inds)
  
  jit(1)  # turn on just-in-time compilation for faster loops
  for (i in 1:K){
    indd[i] <- which(inds == ind_[i])
  }
  indd[length(indd)] <- Nb + 1
  
  # maps parameters to cC matrix in order to recriate matrix cC;
  cC[inds]<- par[3:(Nb+2)]
  # Transform the matrix cC into a matrix of size K \times K
  cC       <- matrix(cC, ncol = K, nrow = K)
  
  # Parameters for EIS iterations:
  
  it      <- 0
  # maximum number of iterations
  itmax   <- 10
  # EIS tolerance to stop iterations
  tol     <- 0.01
  # initial difference
  diff    <- 100
  # initialize vector to save EIS parameters
  bet     <- matrix(0, nrow = Nb+1, ncol = Tesao)                 
  # initialize EIS parameters at zero for initial sampler. See   paragraph bellow eq. (17)
  betas_0 <- bet
  # state at t=0. Initial condition is treated given;
  VINI    <- repmat(Vini,[1,1,Neis]) ###### ESSA LINHA NÃO FUNCIONA # repmat function repeats a matrix, but I'm not sure how -- see: https://www.mathworks.com/help/matlab/ref/repmat.html
  # set space for N draws of EIS parameters at each period t
  XX      <- array(data = NA, dim = c(Neis,Nb+1,T))
  
  # loop for EIS iterations
  while (diff > tol && it < itmax) {
    # Use common random numbers
    s = RandStream('mt19937ar','Seed',semente) # fix seed
    RandStream.setGlobalStream(s)
    
    # Initialize EIS sampler. See eq.(17) --- se if it is right
    # initial state is given in this exercise
    V0     = VINI                                      
    # degrees of freedom of EIS samplers from t=1 to t=T. 
    # Note that for t=T, df_eis=df_p1, since filter estimate is equal to smoothed estimate at T
    # df_eis = [bet(end,:)'+ df_p1; df_p1] ## NOT SURE WHAT IS HAPPENING HERE                 
    # Sample states for EIS regressions:
    jit(1)  # turn on just-in-time compilation for faster loops
    for (i in 1:Tesao){
      # BetMat will be used as the \Gamma matrix from Eq. (17), which collects (k^2+k)/2 auxiliary parameters from the scale matrix of the EIS sampler
      BetMat       <- matrix(0, K)
      # initial sampler starts with zero for EIS parameters;
      BetMat[inds] <- bet[1:Nb,t]
      # t(BetMat * lower.tri(BetMat)) complements the upper triangular part of BetMat 
      #with the same elements below the main diagonal of BetMat
      BetMat <- BetMat + t(BetMat * lower.tri(BetMat))
      # part of the scale matrix of the EIS sampler: \Gamma + yt*yt';
      # The previous part of the code stays outside the loop for N draws, 
      # because EISmat is the same for every replication i
      EISmat <- BetMat + t(yt[t,]) %*% yt[t,]
      
      jit(1)  # turn on just-in-time compilation for faster loops
      for (i in 1:Neis) {
        # scale matrix of the Wishart transition density
        S[,,i,t] <- cC %*% (V0[,,i,t]^d) %*% t(cC) * iv
        # scale matrix of the EIS sampler. See Eq. (17). This code is faster than inv(EISmat+inv(S(:,:,i,t)))
        # wtf?
        Seis[,,i,t] <- S[,,i,t]-1/(1+sum(diag(EISmat %*% S[,,i,t]))) %*% S[,,i,t] %*% EISmat %*% S[,,i,t]
        if (is.positive.definite(Seis[,,i,t], tol=1e-8)) {
          # cholesky decomposition for the scale matrix of the EIS sampler
          cholSeis <- chol(Seis[,,i,t])
          } else {
          # if EIS scale matrix is not positive definite, nearestSPD pick the closest Symmetric Positive Definite matrix to Seis
          Seis[,,i,t]  <- nearPD(Seis[,,i,t]) #### I HAVE TO COME BACK HERE LATER AND WRITE THE FUNCTION
          cholSeis <- chol(Seis[,,i,t])
          }
        if (t > 1) {
          # Compute integrating constant \Chi_{t} to smooth estimates of \Chi_{t-1}. See Eq. (18):
          logdetStm1 <- log(det(Seis[,,i,t]))
          logdetS    <- log(det(S[,,i,t]))
          # log of integrating constant, which is a scalar;
          Y[i, t-1]  <- 0.5 * (df_eis[t] * logdetStm1 - v * logdetS)
        }
        # Sample from EIS sampler
        Veis[,,i,t]  <- rWishart(1, df_eis[t], Seis[,,i,t])
        # Store precision matrices for EIS regressions
        VV           <- Veis[,,i,t]
        # NÃO ENTENDI A SINTAXE ABAIXO
        #XX(i,:,t)     = [-VV(inds)' log(det(VV))];  % EIS regressors. XX is a N x Nb+1 matrix for every period t;
        # compensate main diagonal parameters of XX and log(det(VV));
        XX[i, indd, t] <- XX[i, indd, t]*0.5
        # set the next period V0 precision matrix since the scale matrix S_{t} depends on the precision matrix of the previous period Sigma_{t-1};
      # Why t<T? Because at time T, V0[:,:,i,t] comes from T-1, and the value of V0[:,:,i,T] is not necessary, once the scale matrix S_{T+1} is not simulated;
        if ( t < T) {
          V0[, ,i, t+1] <- Veis[, , i, t]
        }
      }
    }
    # Backward EIS loop to smooth EIS estimates of t (\Chi_{t}) with integrating constant of t+1 (\Chi_{t+1}):
    ####### PAREI NA LINHA 84
  }
  
} # End of the function
```

## Getting everything together with `MSV_EIS_3_simul`

## Compute likelihood multiple times to analyze MC std for varying seeds and fixed parameters

```{r, eval = FALSE}
MC          <- 100
loglikK     <- matrix(0, nrow = MC, ncol = 1)
loglikK_ini <- loglikK
loglikK2    <- loglikK
time        <- loglikK
time_ini    <- time
time2       <- time
adj         <- 7.4
cCinds      <- t(cC[inds])
parreal     <- cbind(d, v, cCinds)
it_print    <- 1
tm1 <- system.time(   # Mimics the usage of tic toc
tic
  {
    jit(1)  # turn on just-in-time compilation for faster loops
    for (j in 1:MC) {
      if (j %% it_print == 0) {
        print(j)
        toc
      }
      seed <- semente + 187*j
      tic
      loglikK_ini <- lik_KChi_initial(parreal, yt, Vini, N, adj, seed) # Initial Sampler
      time_ini[j] <- toc
      tic
      loglikK_ini2[j] <- lik_KChi_R(parreal, yt, Vini, N, adj, seed)
      time2[j]        <- toc # Not sure if this tic toc will work, maybe I'll have to make a workround using system.time later
    }
  }) # Probably this will go wrong because the tic toc in Richard's code is intended to go only until the if part and mine gets the role for() loop. I'll look into this later
```

# Estimating the model with non linear optimizers

The archive `MVS_EIS_3_simul.m` contains two benchmarks for the EIS algorithm.



## Estimate the model with fmincon

## Estimate the model with fminsearch

# References
